{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcFKB0LmIPbC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "from albumentations import (\n",
        "    Compose, RandomCrop, HorizontalFlip, CenterCrop, Normalize,\n",
        ")\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import kagglehub\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_style('darkgrid')\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Download Stanford Dogs Dataset using kagglehub\n",
        "dataset_path = kagglehub.dataset_download(\"jessicali9530/stanford-dogs-dataset\")\n",
        "\n",
        "# Define paths for images and annotations\n",
        "images_dir = os.path.join(dataset_path, 'images/Images')\n",
        "annotations_dir = os.path.join(dataset_path, 'annotations/Annotation')\n",
        "\n",
        "# Verify data integrity\n",
        "if len(os.listdir(annotations_dir)) == len(os.listdir(images_dir)):\n",
        "    print('Number of annotation folders matches the number of image folders.')\n",
        "else:\n",
        "    print('Data mismatch: Annotations and image folders count do not match.')\n",
        "\n",
        "# Analyze image distribution\n",
        "breed_counts = []\n",
        "for breed_folder in os.listdir(images_dir):\n",
        "    breed_name = breed_folder.split('-')[1]\n",
        "    image_count = len(os.listdir(os.path.join(images_dir, breed_folder)))\n",
        "    breed_counts.append((breed_name, image_count))\n",
        "\n",
        "df_breed_counts = pd.DataFrame(breed_counts, columns=['Breed', 'Image Count'])\n",
        "total_images = df_breed_counts['Image Count'].sum()\n",
        "print(f'Total number of images: {total_images}')\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.title('Image Count per Breed (Top 30)')\n",
        "sns.barplot(x='Image Count', y='Breed',\n",
        "            data=df_breed_counts.sort_values('Image Count', ascending=False).head(30))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Create dataframe for training\n",
        "image_data = []\n",
        "for breed_folder in os.listdir(images_dir):\n",
        "    for image_file in os.listdir(os.path.join(images_dir, breed_folder)):\n",
        "        image_path = os.path.join(breed_folder, image_file)\n",
        "        breed_name = breed_folder.split('-')[1]\n",
        "        image_data.append((image_path, breed_name))\n",
        "\n",
        "df_image_data = pd.DataFrame(image_data, columns=['Image Path', 'Breed'])\n",
        "\n",
        "\n",
        "# Augmentation functions\n",
        "def create_train_transforms():\n",
        "    return Compose([\n",
        "        RandomCrop(height=299, width=299, p=1.0),\n",
        "        HorizontalFlip(p=0.5),\n",
        "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ], p=1)\n",
        "\n",
        "\n",
        "def create_val_transforms():\n",
        "    return Compose([\n",
        "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ], p=1)\n",
        "\n",
        "\n",
        "\n",
        "def pca_color_augmentation(image, data_type='Tensor'):\n",
        "    \"\"\"Applies PCA color augmentation to the input image.\"\"\"\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.numpy()\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "\n",
        "    img_reshaped = image.reshape(-1, 3).astype(np.float32)\n",
        "    mean, std = np.mean(img_reshaped, axis=0), np.std(img_reshaped, axis=0)\n",
        "    img_rescaled = (img_reshaped - mean) / std\n",
        "    cov_matrix = np.cov(img_rescaled, rowvar=False)\n",
        "    eigen_vals, eigen_vecs = np.linalg.eig(cov_matrix)\n",
        "    alphas = np.random.normal(loc=0, scale=0.1, size=3)\n",
        "    delta = np.dot(eigen_vecs, alphas * eigen_vals)\n",
        "    pca_aug_image = img_rescaled + delta\n",
        "    pca_aug_image = pca_aug_image * std + mean\n",
        "    aug_image = np.clip(pca_aug_image, 0, 255).astype(np.uint8)\n",
        "\n",
        "    if data_type == 'Tensor':\n",
        "        return torch.from_numpy(np.transpose(aug_image.reshape(image.shape), (2, 0, 1)))\n",
        "    else:\n",
        "        return aug_image.reshape(image.shape)\n",
        "\n",
        "\n",
        "# Dataset class\n",
        "class DogBreedDataset(Dataset):\n",
        "    def __init__(self, image_df, labels_df, transform, mode='train'):\n",
        "        self.image_df = image_df\n",
        "        self.labels_df = labels_df\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(images_dir, self.image_df['Image Path'][idx])\n",
        "        label = self.labels_df.iloc[idx].values\n",
        "\n",
        "        try:\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
        "\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Resize while maintaining aspect ratio, with shorter side = 299\n",
        "            h, w = image.shape[:2]\n",
        "            if h < w:\n",
        "                new_h = 299\n",
        "                new_w = int(w * (299 / h))\n",
        "            else:\n",
        "                new_w = 299\n",
        "                new_h = int(h * (299 / w))\n",
        "\n",
        "            image = cv2.resize(image, (new_w, new_h))\n",
        "\n",
        "\n",
        "            image = CenterCrop(height=299, width=299, p=1)(image=image)['image']\n",
        "\n",
        "            if self.mode == 'train':\n",
        "                if random.random() < 0.5:\n",
        "                    image = pca_color_augmentation(image, data_type='numpy')\n",
        "\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "\n",
        "            return image, label\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "\n",
        "# One-hot encode labels\n",
        "labels_df = pd.get_dummies(df_image_data['Breed'])\n",
        "\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df_image_data, labels_df, test_size=0.25, random_state=SEED, stratify=df_image_data['Breed']\n",
        ")\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_val = X_val.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_val = y_val.reset_index(drop=True)\n",
        "\n",
        "# Create data loaders\n",
        "train_transform = create_train_transforms()\n",
        "val_transform = create_val_transforms()\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_dataset = DogBreedDataset(X_train, y_train, train_transform, mode = 'train')\n",
        "val_dataset = DogBreedDataset(X_val, y_val, val_transform, mode = 'val')\n",
        "\n",
        "# Filter out None values from dataset (due to image loading errors)\n",
        "train_dataset_filtered = [(img, lbl) for img, lbl in train_dataset if img is not None]\n",
        "val_dataset_filtered = [(img, lbl) for img, lbl in val_dataset if img is not None]\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset_filtered, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset_filtered, batch_size=BATCH_SIZE, shuffle =False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Visualization of augmentations (similar to the original code)\n",
        "# ... (Code for visualization remains same)\n",
        "\n",
        "\n",
        "\n",
        "# Model setup and training (using Inception v3)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pre-trained model\n",
        "inception = models.inception_v3(pretrained=True, aux_logits=True).to(device)\n",
        "\n",
        "# Freeze pre-trained layers\n",
        "for param in inception.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the classifier\n",
        "fc_inputs = inception.fc.in_features\n",
        "inception.fc = nn.Sequential(\n",
        "    nn.Linear(fc_inputs, 2048),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(2048, len(labels_df.columns)), # Output layer size should match the number of breeds\n",
        "    nn.LogSoftmax(dim=1)  # Use LogSoftmax for NLLLoss\n",
        ").to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(inception.parameters())\n",
        "\n",
        "\n",
        "# Training loop (simplified)\n",
        "\n",
        "EPOCHS = 4 # Reduced for demonstration\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "\n",
        "    inception.train()  # Set model to training mode\n",
        "    train_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = inception(inputs)\n",
        "        main_output = outputs.logits if hasattr(outputs, 'logits') else outputs  # Handle both old and new Inception versions\n",
        "        loss = criterion(main_output, torch.argmax(labels, dim=1)) # Use argmax to get class indices from one-hot labels\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "\n",
        "    inception.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = inception(inputs)\n",
        "            main_output = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
        "            loss = criterion(main_output, torch.argmax(labels, dim=1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {end_time - start_time:.4f}s\")\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "\n",
        "\n",
        "\n",
        "torch.save(inception.state_dict(), 'inception_finetuning.pth')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "image_path = '/content/drive/MyDrive/Project2/mixed_breeds'\n",
        "\n",
        "valid = []\n",
        "\n",
        "for element in os.listdir(image_path):\n",
        "    # 确保是目录而不是文件\n",
        "    if os.path.isdir(os.path.join(image_path, element)):\n",
        "        breed = element.split('-')[1]\n",
        "        # 过滤掉隐藏文件和非图像文件\n",
        "        images = [img for img in os.listdir(os.path.join(image_path, element)) if not img.startswith('._') and img.endswith(('.jpeg', '.jpg', '.png'))]\n",
        "        valid.append((breed, len(images)))\n",
        "\n",
        "df_test_breeds = pd.DataFrame(valid, columns=['Breeds', 'Number of images'])\n",
        "print('Total number of images :', df_test_breeds['Number of images'].sum())"
      ],
      "metadata": {
        "id": "rge58mQEIhGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for element in os.listdir(image_path):\n",
        "    if os.path.isdir(os.path.join(image_path, element)):\n",
        "        img_files = [img for img in os.listdir(os.path.join(image_path, element)) if img.endswith(('.jpeg', '.jpg', '.png'))]\n",
        "        print(f\"{element}: {len(img_files)} images\")\n",
        "valid = []\n",
        "for element in os.listdir(image_path):\n",
        "    if os.path.isdir(os.path.join(image_path, element)):\n",
        "        img_files = [img for img in os.listdir(os.path.join(image_path, element))\n",
        "                     if not img.startswith('._') and img.endswith(('.jpeg', '.jpg', '.png'))]\n",
        "        valid.extend([(os.path.join(element, img), element.split('-')[1]) for img in img_files])\n",
        "df_test = pd.DataFrame(valid, columns=['Path', 'Label'])\n",
        "print(df_test.head(10))  # 打印前10行\n",
        "print('Total number of valid images:', len(valid))"
      ],
      "metadata": {
        "id": "WhUmLQYSIo3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df_test.join(df_annot, lsuffix='1')\n",
        "df_test = df_test[['Path', 'Label', 'parent_1', 'parent_2']]\n",
        "df_test.head()\n",
        "df_test = df_test.replace('shih-tzu', 'shih')\n",
        "df_test = df_test.replace('german_short-haired_pointer', 'german_short')\n"
      ],
      "metadata": {
        "id": "HFlJavlOIrQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use same labels as full standford dog dataset to allow matching parent breed during model testing\n",
        "d1 = pd.DataFrame(0, index=np.arange(len(df_test)), columns=labels.columns)\n",
        "d1.columns= d1.columns.str.lower()\n",
        "d2 = pd.DataFrame(0, index=np.arange(len(df_test)), columns=labels.columns)\n",
        "d2.columns= d2.columns.str.lower()\n",
        "parent1_labels = pd.get_dummies(df_test['parent_1'])\n",
        "parent2_labels = pd.get_dummies(df_test['parent_2'])\n",
        "d1 = d1.add(parent1_labels).fillna(0)\n",
        "d2 = d2.add(parent2_labels).fillna(0)\n",
        "test_labels = d1.add(d2)\n",
        "idx2breed = dict(enumerate(test_labels.columns))"
      ],
      "metadata": {
        "id": "obYfkdc2ItCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = loader(df_test, test_labels, batch_size = BATCH_SIZE, obj = 'test')\n",
        "# Load the saved model\n",
        "PATH = '/content/drive/MyDrive/inception_finetuningphr.pth'\n",
        "inception = models.inception_v3(pretrained = True, aux_logits=True)\n",
        "# Freeze model parameters\n",
        "for param in inception.parameters():\n",
        "    param.requires_grad = False\n",
        "# Change the final layer of Inception Model for Transfer Learning\n",
        "fc_inputs = inception.fc.in_features\n",
        "inception.fc = nn.Sequential(\n",
        "    nn.Linear(fc_inputs, 2048),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(2048, 120),\n",
        "    nn.LogSoftmax(dim=1) # For using NLLLoss()\n",
        ")\n",
        "inception.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))\n",
        "inception.eval()\n"
      ],
      "metadata": {
        "id": "pY7hj9iRIuhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_size = len(df_test)\n",
        "with torch.no_grad():\n",
        "    both_parents_acc = 0.0\n",
        "    at_least_1_parent_acc = 0.0\n",
        "    for j, (inputs, labels) in enumerate(test_loader):\n",
        "        parents_true = np.argpartition(labels, -2)[:,-2:].detach().numpy()\n",
        "        outputs = inception(inputs)\n",
        "        parents_pred = np.argpartition(outputs.detach().numpy(), -2)[:,-2:]\n",
        "\n",
        "        for pred, true in zip(parents_pred, parents_true):\n",
        "            both_parents_acc += len(np.intersect1d(pred, true))/2\n",
        "\n",
        "        for pred, true in zip(parents_pred, parents_true):\n",
        "            at_least_1_parent_acc += float(np.any(np.in1d(pred,true)))\n",
        "\n",
        "both_parents_acc = both_parents_acc/float(test_data_size)\n",
        "at_least_1_parent_acc = at_least_1_parent_acc/float(test_data_size)\n",
        "\n",
        "print('avg accuracy in detecting both parents: ', both_parents_acc, '\\navg accuracy in detecting at least one parent:', at_least_1_parent_acc)"
      ],
      "metadata": {
        "id": "q27Iyq7hIy7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def topKone(loader, data_size, k):\n",
        "  at_least_one = 0.0\n",
        "\n",
        "    for j, (inputs, labels) in enumerate(loader):\n",
        "        parents_true = np.argpartition(labels, -2)[:,-2:].detach().numpy()\n",
        "        outputs = inception(inputs)\n",
        "        parents_pred = np.argpartition(outputs.detach().numpy(), -k)[:,-k:]\n",
        "\n",
        "        counts = np.array([sum(label in pred for label in true) for pred, true in zip(parents_pred, parents_true)])\n",
        "        at_least_one += float(sum(counts == 1))\n",
        "    return at_least_one / float(data_size)\n",
        "def topKtwo(loader, data_size, k):\n",
        "\n",
        "    both_parents = 0.0\n",
        "    for j, (inputs, labels) in enumerate(loader):\n",
        "        parents_true = np.argpartition(labels, -2)[:,-2:].detach().numpy()\n",
        "        outputs = inception(inputs)\n",
        "        parents_pred = np.argpartition(outputs.detach().numpy(), -k)[:,-k:]\n",
        "\n",
        "        counts = np.array([sum(label in pred for label in true) for pred, true in zip(parents_pred, parents_true)])\n",
        "        both_parents += float(sum(counts == 2))\n",
        "    return both_parents / float(data_size)"
      ],
      "metadata": {
        "id": "HzIvgS1FI1qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_values = [5, 10, 20, 30, 40, 50]\n",
        "topKone_acc = [topKone(test_loader, test_data_size, k=k) for k in k_values]\n",
        "topKtwo_acc = [topKtwo(test_loader, test_data_size, k=k) for k in k_values]\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
        "\n",
        "ax[0].plot(k_values, topKone_acc)\n",
        "ax[0].set_title('One Parent Accuracy')\n",
        "ax[0].set_xlabel('K values')\n",
        "ax[0].set_ylabel('Accuracy')\n",
        "ax[1].plot(k_values, topKtwo_acc)\n",
        "ax[1].set_title('Two Parents Accuracy')\n",
        "ax[1].set_xlabel('K values')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iv1mdXh5I_DM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}