# Petoria（Model）

## Project Overview
This is model part(Flask back-end) of Petoria project which is a chatbot that can chat with user about dog and identify the breed of a dog in picture.

## Project Structure
```
.
├── .venv/                 # Python virtual environment folder
├── model training code       # Training code for model(important!)
├── app.py                  # Main application
├── Dockerfile             # Docker configuration file
├── inception_finetuning.pth # Pretrained weights for the Inception model
├── requirements.txt       # Python dependencies list
```

---
## Model Link(IMPORTANT!)
Please download our trained model and put it in root directory of this project.
Model link：https://drive.google.com/file/d/1tYLDyIrDi0rfrv0vclheh5TmGlyqI7MD/view?usp=sharing
## Running with Docker(Recommend)

1. **Build the Docker Image**

   In the project's root directory, use the following command to build the Docker image:

   ```bash
   docker build -t project-image .
   ```

2. **Run the Docker Container**

   After building, use the following command to run the Docker container:

   ```bash
   docker run -e GEMINI_API_KEY=your Google_API_KEY -p 5000:5000 project-image
   ```

   This will start the Flask API on port 5000. You can access the API at `http://localhost:5000`.
   
   you can get your Google_API_KEY in this website:https://aistudio.google.com/app/apikey

---

## Running Locally (VSCode + .venv)

1. **Create the Virtual Environment**

   In the project's root directory, run the following command to create a `.venv` virtual environment:

   ```bash
   python -m venv .venv
   ```

2. **Activate the Virtual Environment**

   - On Windows:
     ```bash
     .venv\Scripts\activate
     ```
   - On macOS/Linux:
     ```bash
     source .venv/bin/activate
     ```

3. **Install Dependencies**

   After activating the virtual environment, run the following command to install project dependencies:

   ```bash
   pip install -r requirements.txt
   ```

4. **Run the Project**

   Remember to set your GEMINI_API_KEY in your environment.

   Navigate to the `app` directory and run the main program:

   ```bash
   python app.py
   ```

   The project will start on port 5000 locally, and you can access the API at `http://localhost:5000`.

---

## API Endpoints

The project provides two main API endpoints, accessible on **port 5000** by default:

### 1. Image Processing Endpoint (`/process_image`)
- **Method**: `POST`
- **Description**: Accepts an image file of a dog and identifies the breed. If the dog is determined to be a purebred, the endpoint will return a single breed prediction. If the dog is likely a mixed breed, it will return the top two possible parent breeds.
- **Input**: 
  - Form-data with an image file (`file` parameter), preferably in JPEG or PNG format.
- **Response**:
  - `message`: Confirmation that the image was successfully processed.
  - `breed_type`: Indicates if the dog is predicted as "Purebred" or "Mixed breed" based on a confidence threshold.
  - `purebred_prediction`: The predicted breed if the dog is identified as a purebred.
  - `mixed_breed_prediction`: An array of the top two possible breeds if identified as a mixed breed.

  **Sample Response**:
  ```json
  {
    "message": "Image processed successfully!",
    "breed_type": "Purebred",
    "purebred_prediction": "golden_retriever",
    "mixed_breed_prediction": ["golden_retriever", "labrador_retriever"]
  }
  ```

### 2. Chat Endpoint (`/chat`)
- **Method**: `POST`
- **Description**: This endpoint allows users to chat with the bot about pet-related topics. The chatbot has a friendly tone and can discuss different dog breeds and general pet care information.
- **Input**: JSON payload with an `"input"` key containing the user's message.
- **Response**:
  - `response`: A text reply generated by the chatbot based on the user's input.

  **Sample Response**:
  ```json
  {
    "response": "Golden Retrievers are known for their friendly and gentle temperament. Do you own one?"
  }
  ```

### Running the API

To access these endpoints, start the application (using Docker or a local environment as described above), and the API will be available at `http://localhost:5000`.

---

## Notes

- Ensure that the `inception_finetuning.pth` file is located in the project root, and that the model code correctly loads this file.
- If you need to make code adjustments, install the required dependencies in the `.venv` environment and set the Python interpreter in VSCode to point to `.venv`.

---

